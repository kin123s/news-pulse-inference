# K8s ë°°í¬ ê°€ì´ë“œ - GPU ë¦¬ì†ŒìŠ¤ ê³ ë ¤

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. Prerequisites

```bash
# Kubernetes í´ëŸ¬ìŠ¤í„° (Minikube, K3s, GKE, EKS, AKS ë“±)
kubectl version

# Helm (ì„ íƒì‚¬í•­)
helm version

# Metrics Server (HPAë¥¼ ìœ„í•´ í•„ìš”)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

### 2. Namespace ìƒì„±

```bash
kubectl apply -f namespace.yaml
```

### 3. Secret ì„¤ì •

```bash
# API í‚¤ ì„¤ì •
kubectl create secret generic api-secrets \
  --from-literal=openai-api-key=YOUR_OPENAI_KEY \
  --from-literal=news-api-key=YOUR_NEWS_API_KEY \
  -n news-analysis

# ë˜ëŠ” YAML íŒŒì¼ ìˆ˜ì • í›„ ì ìš©
kubectl apply -f secret.yaml
```

### 4. ì¸í”„ë¼ ë°°í¬

```bash
# Kafka
kubectl apply -f kafka-deployment.yaml

# Redis
kubectl apply -f redis-deployment.yaml

# Prometheus (ëª¨ë‹ˆí„°ë§)
kubectl apply -f prometheus-deployment.yaml
```

### 5. ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬

```bash
# Producer
kubectl apply -f producer-deployment.yaml

# Consumer
kubectl apply -f consumer-deployment.yaml

# Inference Server (NEW!)
kubectl apply -f inference-deployment.yaml
```

### 6. ë°°í¬ í™•ì¸

```bash
# Pod ìƒíƒœ í™•ì¸
kubectl get pods -n news-analysis

# ì„œë¹„ìŠ¤ í™•ì¸
kubectl get svc -n news-analysis

# HPA ìƒíƒœ í™•ì¸
kubectl get hpa -n news-analysis

# ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/inference-server -n news-analysis
```

---

## ğŸ¯ GPU ë¦¬ì†ŒìŠ¤ ì„¤ì •

### GPU ë…¸ë“œ ì„¤ì • (GKE ì˜ˆì‹œ)

```bash
# GPU ë…¸ë“œ í’€ ìƒì„±
gcloud container node-pools create gpu-pool \
  --cluster=news-analysis-cluster \
  --accelerator type=nvidia-tesla-t4,count=1 \
  --machine-type=n1-standard-4 \
  --num-nodes=1 \
  --min-nodes=0 \
  --max-nodes=3 \
  --enable-autoscaling

# NVIDIA GPU Operator ì„¤ì¹˜
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/gpu-operator/master/deployments/gpu-operator.yaml
```

### GPU ë¦¬ì†ŒìŠ¤ ìš”ì²­ í™œì„±í™”

[inference-deployment.yaml](inference-deployment.yaml)ì—ì„œ ì•„ë˜ ë¶€ë¶„ ì£¼ì„ í•´ì œ:

```yaml
resources:
  requests:
    nvidia.com/gpu: "1"
  limits:
    nvidia.com/gpu: "1"

nodeSelector:
  accelerator: nvidia-tesla-t4

tolerations:
- key: nvidia.com/gpu
  operator: Exists
  effect: NoSchedule
```

### GPU ì‚¬ìš© í™•ì¸

```bash
# GPU ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl describe nodes | grep -A 5 "Allocated resources"

# GPU ì‚¬ìš© ì¤‘ì¸ Pod í™•ì¸
kubectl get pods -n news-analysis -o wide
```

---

## ğŸ“Š HPA (Horizontal Pod Autoscaling)

### Metrics Server ì„¤ì¹˜ í™•ì¸

```bash
kubectl get deployment metrics-server -n kube-system
```

### HPA ì„¤ì •

í˜„ì¬ HPAëŠ” ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§:

1. **CPU ì‚¬ìš©ë¥ **: 70% ì´ìƒ
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ **: 80% ì´ìƒ
3. **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­**: ì´ˆë‹¹ ì¶”ë¡  ìš”ì²­ ìˆ˜

### ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ (Prometheus Adapter)

```bash
# Prometheus Adapter ì„¤ì¹˜
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus-adapter prometheus-community/prometheus-adapter \
  --namespace news-analysis \
  --set prometheus.url=http://prometheus:9090 \
  --set rules.default=false \
  --set-file rules.custom=prometheus-adapter-rules.yaml
```

### ë¶€í•˜ í…ŒìŠ¤íŠ¸

```bash
# kubectl runìœ¼ë¡œ ë¶€í•˜ ìƒì„±
kubectl run -it --rm load-generator \
  --image=busybox \
  --restart=Never \
  -n news-analysis \
  -- /bin/sh -c "while true; do wget -q -O- http://inference-server:8000/health; done"

# HPA ìŠ¤ì¼€ì¼ë§ í™•ì¸
kubectl get hpa inference-server-hpa -n news-analysis --watch
```

---

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### Prometheus ë©”íŠ¸ë¦­ í™•ì¸

```bash
# Prometheus UI ì ‘ì†
kubectl port-forward svc/prometheus -n news-analysis 9090:9090

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:9090 ì ‘ì†
```

ì£¼ìš” ë©”íŠ¸ë¦­:
- `inference_requests_total`: ì´ ì¶”ë¡  ìš”ì²­ ìˆ˜
- `inference_duration_seconds`: ì¶”ë¡  ì²˜ë¦¬ ì‹œê°„
- `inference_batch_size`: ë°°ì¹˜ í¬ê¸°
- `active_inference_tasks`: ì‹¤í–‰ ì¤‘ì¸ ì¶”ë¡  ì‘ì—… ìˆ˜
- `kafka_consumer_lag`: Kafka ì»¨ìŠˆë¨¸ ì§€ì—°

### Grafana ëŒ€ì‹œë³´ë“œ (ì„ íƒì‚¬í•­)

```bash
# Grafana ì„¤ì¹˜
helm install grafana grafana/grafana -n news-analysis

# Admin ë¹„ë°€ë²ˆí˜¸ í™•ì¸
kubectl get secret grafana -n news-analysis -o jsonpath="{.data.admin-password}" | base64 --decode

# Grafana ì ‘ì†
kubectl port-forward svc/grafana -n news-analysis 3000:3000
```

### ë¡œê·¸ í™•ì¸

```bash
# íŠ¹ì • Pod ë¡œê·¸
kubectl logs -f <pod-name> -n news-analysis

# ëª¨ë“  inference server ë¡œê·¸
kubectl logs -f deployment/inference-server -n news-analysis

# ì—ëŸ¬ë§Œ í•„í„°ë§
kubectl logs deployment/inference-server -n news-analysis | grep ERROR
```

---

## ğŸ”„ ì—…ë°ì´íŠ¸ ë° ë¡¤ë°±

### Rolling Update

```bash
# ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸
kubectl set image deployment/inference-server \
  inference=news-inference:v2.0 \
  -n news-analysis

# ë¡¤ì•„ì›ƒ ìƒíƒœ í™•ì¸
kubectl rollout status deployment/inference-server -n news-analysis
```

### ë¡¤ë°±

```bash
# ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
kubectl rollout undo deployment/inference-server -n news-analysis

# íŠ¹ì • ë¦¬ë¹„ì „ìœ¼ë¡œ ë¡¤ë°±
kubectl rollout undo deployment/inference-server --to-revision=2 -n news-analysis

# ë¡¤ì•„ì›ƒ íˆìŠ¤í† ë¦¬
kubectl rollout history deployment/inference-server -n news-analysis
```

---

## ğŸ’¾ ë¦¬ì†ŒìŠ¤ ìµœì í™”

### Vertical Pod Autoscaler (VPA)

```bash
# VPA ì„¤ì¹˜
kubectl apply -f https://github.com/kubernetes/autoscaler/releases/download/vertical-pod-autoscaler-0.14.0/vpa-v0.14.0.yaml

# VPA ì„¤ì •
kubectl apply -f - <<EOF
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: inference-server-vpa
  namespace: news-analysis
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: inference-server
  updatePolicy:
    updateMode: "Auto"
EOF
```

### Pod Disruption Budget

```bash
kubectl apply -f - <<EOF
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: inference-server-pdb
  namespace: news-analysis
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: inference-server
EOF
```

---

## ğŸ§ª ì„±ëŠ¥ íŠœë‹

### ëŒ€ëŸ‰ ìœ ì… ì‹œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì„¤ì •

í˜„ì¬ HPA ì„¤ì •:
- **ìµœì†Œ ë ˆí”Œë¦¬ì¹´**: 2
- **ìµœëŒ€ ë ˆí”Œë¦¬ì¹´**: 20
- **ìŠ¤ì¼€ì¼ì—…**: ì¦‰ì‹œ (ì•ˆì •í™” 0ì´ˆ)
- **ìŠ¤ì¼€ì¼ë‹¤ìš´**: 5ë¶„ ì•ˆì •í™”

ëŒ€ëŸ‰ íŠ¸ë˜í”½ ì˜ˆìƒ ì‹œ:

```bash
# ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§
kubectl scale deployment inference-server --replicas=10 -n news-analysis

# HPA ìµœì†Œê°’ ì¦ê°€
kubectl patch hpa inference-server-hpa -n news-analysis \
  --patch '{"spec":{"minReplicas":5}}'
```

### ë¦¬ì†ŒìŠ¤ ì œí•œ ì¡°ì •

ê³ ë¶€í•˜ í™˜ê²½:
```yaml
resources:
  requests:
    memory: "4Gi"
    cpu: "2000m"
  limits:
    memory: "8Gi"
    cpu: "4000m"
```

ì €ë¶€í•˜ í™˜ê²½:
```yaml
resources:
  requests:
    memory: "1Gi"
    cpu: "500m"
  limits:
    memory: "2Gi"
    cpu: "1000m"
```

---

## ğŸ”’ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### Network Policies

```bash
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: inference-server-netpol
  namespace: news-analysis
spec:
  podSelector:
    matchLabels:
      app: inference-server
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: consumer
    ports:
    - protocol: TCP
      port: 8000
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: kafka
    ports:
    - protocol: TCP
      port: 9093
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
EOF
```

---

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [Kubernetes GPU ê°€ì´ë“œ](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)
- [HPA ìƒì„¸ ë¬¸ì„œ](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [Prometheus Adapter](https://github.com/kubernetes-sigs/prometheus-adapter)
- [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html)
